{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional Lab: Cost Function for Logistic Regression\n",
    "\n",
    "## Goals\n",
    "In this lab, you will:\n",
    "- examine the implementation and utilize the cost function for logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lab_utils_common'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlab_utils_common\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m  plot_data, sigmoid, dlc\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'lab_utils_common'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from lab_utils_common import  plot_data, sigmoid, dlc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset \n",
    "Let's start with the same dataset as was used in the decision boundary lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = np.array([[0.5, 1.5], [1,1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])  #(m,n)\n",
    "y_train = np.array([0, 0, 0, 1, 1, 1])                                           #(m,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a helper function to plot this data. The data points with label $y=1$ are shown as red crosses, while the data points with label $y=0$ are shown as blue circles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m fig,ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m,figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m4\u001b[39m))\n\u001b[1;32m----> 2\u001b[0m \u001b[43mplot_data\u001b[49m(X_train, y_train, ax)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Set both axes to be from 0-4\u001b[39;00m\n\u001b[0;32m      5\u001b[0m ax\u001b[38;5;241m.\u001b[39maxis([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3.5\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plot_data' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig,ax = plt.subplots(1,1,figsize=(4,4))\n",
    "plot_data(X_train, y_train, ax)\n",
    "\n",
    "# Set both axes to be from 0-4\n",
    "ax.axis([0, 4, 0, 3.5])\n",
    "ax.set_ylabel('$x_1$', fontsize=12)\n",
    "ax.set_xlabel('$x_0$', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function\n",
    "\n",
    "In a previous lab, you developed the *logistic loss* function. Recall, loss is defined to apply to one example. Here you combine the losses to form the **cost**, which includes all the examples.\n",
    "\n",
    "\n",
    "Recall that for logistic regression, the cost function is of the form \n",
    "\n",
    "$$ J(\\mathbf{w},b) = \\frac{1}{m} \\sum_{i=0}^{m-1} \\left[ loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) \\right] \\tag{1}$$\n",
    "\n",
    "where\n",
    "* $loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)})$ is the cost for a single data point, which is:\n",
    "\n",
    "    $$loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) = -y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) \\tag{2}$$\n",
    "    \n",
    "*  where m is the number of training examples in the data set and:\n",
    "$$\n",
    "\\begin{align}\n",
    "  f_{\\mathbf{w},b}(\\mathbf{x^{(i)}}) &= g(z^{(i)})\\tag{3} \\\\\n",
    "  z^{(i)} &= \\mathbf{w} \\cdot \\mathbf{x}^{(i)}+ b\\tag{4} \\\\\n",
    "  g(z^{(i)}) &= \\frac{1}{1+e^{-z^{(i)}}}\\tag{5} \n",
    "\\end{align}\n",
    "$$\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex-02'></a>\n",
    "#### Code Description\n",
    "\n",
    "The algorithm for `compute_cost_logistic` loops over all the examples calculating the loss for each example and accumulating the total.\n",
    "\n",
    "Note that the variables X and y are not scalar values but matrices of shape ($m, n$) and ($𝑚$,) respectively, where  $𝑛$ is the number of features and $𝑚$ is the number of training examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_logistic(X, y, w, b):\n",
    "    \"\"\"\n",
    "    Computes cost\n",
    "\n",
    "    Args:\n",
    "      X (ndarray (m,n)): Data, m examples with n features\n",
    "      y (ndarray (m,)) : target values\n",
    "      w (ndarray (n,)) : model parameters  \n",
    "      b (scalar)       : model parameter\n",
    "      \n",
    "    Returns:\n",
    "      cost (scalar): cost\n",
    "    \"\"\"\n",
    "\n",
    "    m = X.shape[0]\n",
    "    cost = 0.0\n",
    "    for i in range(m):\n",
    "        z_i = np.dot(X[i],w) + b\n",
    "        f_wb_i = sigmoid(z_i)\n",
    "        cost +=  -y[i]*np.log(f_wb_i) - (1-y[i])*np.log(1-f_wb_i)\n",
    "             \n",
    "    cost = cost / m\n",
    "    return cost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the implementation of the cost function using the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36686678640551745\n"
     ]
    }
   ],
   "source": [
    "w_tmp = np.array([1,1])\n",
    "b_tmp = -3\n",
    "print(compute_cost_logistic(X_train, y_train, w_tmp, b_tmp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**: 0.3668667864055175"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "Now, let's see what the cost function output is for a different value of $w$. \n",
    "\n",
    "* In a previous lab, you plotted the decision boundary for  $b = -3, w_0 = 1, w_1 = 1$. That is, you had `b = -3, w = np.array([1,1])`.\n",
    "\n",
    "* Let's say you want to see if $b = -4, w_0 = 1, w_1 = 1$, or `b = -4, w = np.array([1,1])` provides a better model.\n",
    "\n",
    "Let's first plot the decision boundary for these two different $b$ values to see which one fits the data better.\n",
    "\n",
    "* For $b = -3, w_0 = 1, w_1 = 1$, we'll plot $-3 + x_0+x_1 = 0$ (shown in blue)\n",
    "* For $b = -4, w_0 = 1, w_1 = 1$, we'll plot $-4 + x_0+x_1 = 0$ (shown in magenta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c4d0f7cefdd4deb90ad46bd84aa9da2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Choose values between 0 and 6\n",
    "x0 = np.arange(0,6)\n",
    "\n",
    "# Plot the two decision boundaries\n",
    "x1 = 3 - x0\n",
    "x1_other = 4 - x0\n",
    "\n",
    "fig,ax = plt.subplots(1, 1, figsize=(4,4))\n",
    "# Plot the decision boundary\n",
    "ax.plot(x0,x1, c=dlc[\"dlblue\"], label=\"$b$=-3\")\n",
    "ax.plot(x0,x1_other, c=dlc[\"dlmagenta\"], label=\"$b$=-4\")\n",
    "ax.axis([0, 4, 0, 4])\n",
    "\n",
    "# Plot the original data\n",
    "plot_data(X_train,y_train,ax)\n",
    "ax.axis([0, 4, 0, 4])\n",
    "ax.set_ylabel('$x_1$', fontsize=12)\n",
    "ax.set_xlabel('$x_0$', fontsize=12)\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.title(\"Decision Boundary\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see from this plot that `b = -4, w = np.array([1,1])` is a worse model for the training data. Let's see if the cost function implementation reflects this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost for b = -3 :  0.36686678640551745\n",
      "Cost for b = -4 :  0.5036808636748461\n"
     ]
    }
   ],
   "source": [
    "w_array1 = np.array([1,1])\n",
    "b_1 = -3\n",
    "w_array2 = np.array([1,1])\n",
    "b_2 = -4\n",
    "\n",
    "print(\"Cost for b = -3 : \", compute_cost_logistic(X_train, y_train, w_array1, b_1))\n",
    "print(\"Cost for b = -4 : \", compute_cost_logistic(X_train, y_train, w_array2, b_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**\n",
    "\n",
    "Cost for b = -3 :  0.3668667864055175\n",
    "\n",
    "Cost for b = -4 :  0.5036808636748461\n",
    "\n",
    "\n",
    "You can see the cost function behaves as expected and the cost for `b = -4, w = np.array([1,1])` is indeed higher than the cost for `b = -3, w = np.array([1,1])`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations!\n",
    "In this lab you examined and utilized the cost function for logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
